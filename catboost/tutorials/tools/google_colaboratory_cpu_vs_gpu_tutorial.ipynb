{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of colaboratory_cpu_vs_gpu_tutorial.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/catboost/tutorials/blob/master/tools/google_colaboratory_cpu_vs_gpu_tutorial.ipynb","timestamp":1550035598487},{"file_id":"1iLzXm9f0oWYkukSgAh-9silP2T9MpERH","timestamp":1545215978704},{"file_id":"1yFVSjdR2nIWeQvOd5-rMdtLRHcI3lx-0","timestamp":1545214576187}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"text","id":"q1dEw-XVkzUT"},"cell_type":"markdown","source":["# Gradient Boosting: CPU vs GPU\n","This is a basic tuturoal which shows how to run gradient boosting on CPU and GPU on Google Colaboratory. It will give you an opportunity to see the speedup that you get from GPU training. The speedup is large even on Tesla K80 that is available in Colaboratory. On newer generations of GPU the speedup will be much bigger.\n","\n","We will use CatBoost gradient boosting library, which is known for it's good GPU performance.\n","  \n"," You could try it out on Colaboratory, just pressing on the following badge:  \n"," \n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catboost/tutorials/blob/master/tools/google_colaboratory_cpu_vs_gpu_tutorial.ipynb) "]},{"metadata":{"colab_type":"text","id":"wPOIzC8amcEk"},"cell_type":"markdown","source":["## Set GPU as hardware accelerator\n","First of all, you need to select GPU as hardware accelerator. There are two simple steps to do so:  \n","Step 1. Navigate to 'Runtime' menu and select 'Change runtime type'  \n","Step 2. Choose GPU as hardware accelerator.  \n","That's all!"]},{"metadata":{"colab_type":"text","id":"SvkCBNqRkX1t"},"cell_type":"markdown","source":["## Importing CatBoost\n","\n","Next big thing is to import CatBoost inside environment. Colaboratory has built in libraries installed and most libraries can be installed quickly with a simple *!pip install* command.  \n","Please ignore the warning message about already imported enum package. Furthermore take note that you need to re-import the library every time you start a new session of Colab."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1550032443744,"user_tz":-420,"elapsed":10847,"user":{"displayName":"","photoUrl":"","userId":""}},"id":"5A8e1XP3kqTx","outputId":"9ec4997b-4f0c-4dc2-f89a-1a6fd19bb31e","colab":{"base_uri":"https://localhost:8080/","height":342}},"cell_type":"code","source":["!pip install catboost"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting catboost\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/03/777a0e1c12571a7f3320a4fa6d5f123dba2dd7c0bca34f4f698a6396eb48/catboost-0.12.2-cp36-none-manylinux1_x86_64.whl (55.5MB)\n","\u001b[K    100% |████████████████████████████████| 55.5MB 968kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.11.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.14.6)\n","Collecting enum34 (from catboost)\n","  Downloading https://files.pythonhosted.org/packages/af/42/cb9355df32c69b553e72a2e28daee25d1611d2c0d9c272aa1d34204205b2/enum34-1.1.6-py3-none-any.whl\n","Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.22.0)\n","Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2.5.3)\n","Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.1->catboost) (2018.9)\n","Installing collected packages: enum34, catboost\n","Successfully installed catboost-0.12.2 enum34-1.1.6\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["enum"]}}},"metadata":{"tags":[]}}]},{"metadata":{"colab_type":"text","id":"UKnT_rYWWhD8"},"cell_type":"markdown","source":["##Download and prepare dataset\n","The next step is dataset downloading. GPU training is useful for large datsets. You will get a good speedup starting from 10k objects and the more objects you have, the more will be the speedup.\n","Because of that reason we have selected a large dataset - [Epsilon](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html) (500.000 documents and 2.000 features) for this tutorial.\n","Firstly, we will get the data through catboost.datasets module. The code below does this. It will run for approximately 10-15 minutes. So please be patient :)"]},{"metadata":{"colab_type":"code","id":"lzWG0GeAHVg9","colab":{}},"cell_type":"code","source":["from catboost.datasets import epsilon\n","\n","train, test = epsilon()\n","\n","X_train, y_train = train.iloc[:,1:], train[0]\n","X_test, y_test = train.iloc[:,1:], train[0]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"5JhczRSqBFnd"},"cell_type":"markdown","source":["## Training on CPU\n","Now we will train the model on CPU and measure execution time.\n","We will use 100 iterations for our CPU training since otherwise it will take a long time.\n","It will take around 15 minutes."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1550035117842,"user_tz":-420,"elapsed":862812,"user":{"displayName":"","photoUrl":"","userId":""}},"id":"3wuVZsTGC7WO","outputId":"4e857ff6-8a60-490e-e16a-ff8cfab7b448","colab":{"base_uri":"https://localhost:8080/","height":197}},"cell_type":"code","source":["from catboost import CatBoostClassifier\n","import timeit\n","\n","def train_on_cpu():  \n","  model = CatBoostClassifier(\n","    iterations=100,\n","    learning_rate=0.03,\n","    eval_metric='Accuracy',\n","  )\n","  \n","  model.fit(\n","      X_train, y_train,\n","      eval_set=(X_test, y_test),\n","      verbose=30\n","  );   \n","      \n","cpu_time = timeit.timeit('train_on_cpu()', setup=\"from __main__ import train_on_cpu\", number=1)\n","\n","print('Time to fit model on CPU: {} sec'.format(int(cpu_time)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["0:\tlearn: 0.6648425\ttest: 0.6648425\tbest: 0.6648425 (0)\ttotal: 7.12s\tremaining: 11m 45s\n","30:\tlearn: 0.7375125\ttest: 0.7375125\tbest: 0.7375125 (30)\ttotal: 3m 34s\tremaining: 7m 56s\n","60:\tlearn: 0.7686725\ttest: 0.7686725\tbest: 0.7686725 (60)\ttotal: 6m 48s\tremaining: 4m 21s\n","90:\tlearn: 0.7885500\ttest: 0.7885500\tbest: 0.7885500 (90)\ttotal: 9m 58s\tremaining: 59.2s\n","99:\tlearn: 0.7932025\ttest: 0.7932025\tbest: 0.7932025 (99)\ttotal: 10m 55s\tremaining: 0us\n","\n","bestTest = 0.7932025\n","bestIteration = 99\n","\n","Time to fit model on CPU: 862 sec\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"m0EafIhX-SgH"},"cell_type":"markdown","source":["Take notice that learning time itself wothout data feeding is around 12 minutes. Whereas all the process consumes 14-15 min."]},{"metadata":{"colab_type":"text","id":"C1jsWgxhCLTX"},"cell_type":"markdown","source":["## Training on GPU\n","The previous code execution has been done on CPU. It's time to use GPU!  \n","We need to use '*task_type='GPU'*' parameter value to run GPU training. Now the execution time wouldn't be so big :)  \n","BTW if Colaboratory shows you a warning 'GPU memory usage is close to the limit', just press 'Ignore'."]},{"metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1550035444804,"user_tz":-420,"elapsed":196293,"user":{"displayName":"","photoUrl":"","userId":""}},"id":"Oq4Lpx1veXnI","outputId":"2f9f7392-fdda-498a-d643-e9753b1ecc8b","colab":{"base_uri":"https://localhost:8080/","height":179}},"cell_type":"code","source":["def train_on_gpu():  \n","  model = CatBoostClassifier(\n","    iterations=100,\n","    learning_rate=0.03,\n","    eval_metric='Accuracy',\n","    task_type='GPU'\n","  )\n","  \n","  model.fit(\n","      X_train, y_train,\n","      eval_set=(X_test, y_test),\n","      verbose=30\n","  );     \n","      \n","gpu_time = timeit.timeit('train_on_gpu()', setup=\"from __main__ import train_on_gpu\", number=1)\n","\n","print('Time to fit model on GPU: {} sec'.format(int(gpu_time)))\n","print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0:\tlearn: 0.6652575\ttest: 0.6652575\tbest: 0.6652575 (0)\ttotal: 323ms\tremaining: 32s\n","30:\tlearn: 0.7377725\ttest: 0.7377725\tbest: 0.7377725 (30)\ttotal: 6.44s\tremaining: 14.3s\n","60:\tlearn: 0.7700775\ttest: 0.7700775\tbest: 0.7700775 (60)\ttotal: 12.2s\tremaining: 7.81s\n","90:\tlearn: 0.7893275\ttest: 0.7893275\tbest: 0.7893275 (90)\ttotal: 17.9s\tremaining: 1.77s\n","99:\tlearn: 0.7937800\ttest: 0.7937800\tbest: 0.7937800 (99)\ttotal: 19.5s\tremaining: 0us\n","bestTest = 0.79378\n","bestIteration = 99\n","Time to fit model on GPU: 195 sec\n","GPU speedup over CPU: 4x\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"fp4-J4YlbDxF"},"cell_type":"markdown","source":["As you can see GPU is much faster than CPU on large datasets. It takes just 3-4 mins vs 14-15 mins to fit the model. Moreover learning process consumes just 30 seconds vs 12 minutes! This is a good reason to use GPU instead of CPU!\n","  \n","Thank you for attention! "]}]}